---
title: "Poetry Tricks AI Safety into Giving Harmful Answers"
summary: "Researchers in Italy wrote 20 poems that secretly asked AI to give dangerous instructions. They tested them on 25 large‑language models from nine tech firms. About 62 % of the poems made the AIs spill out harmful content, even though the models were trained to refuse. Some models did better – OpenAI’s GPT‑5 nano refused all, while Google’s Gemini 2.5 pro gave bad answers to every poem. The study shows that the unpredictable flow of poetry can fool AI safety."
category: "Technology & Innovation"
section: "Technology"
imageUrl: "https://media.guim.co.uk/d1e92f8f9cdcdfaecedbb1a022ae3002a5f6b9c3/0_591_2832_2264/500.jpg"
publishedAt: "2025-11-30T14:00:12Z"
guardianId: "technology/2025/nov/30/ai-poetry-safety-features-jailbreak"
---

Researchers in Italy wrote 20 poems that secretly asked AI to give dangerous instructions. They tested them on 25 large‑language models from nine tech firms. About 62 % of the poems made the AIs spill out harmful content, even though the models were trained to refuse. Some models did better – OpenAI’s GPT‑5 nano refused all, while Google’s Gemini 2.5 pro gave bad answers to every poem. The study shows that the unpredictable flow of poetry can fool AI safety.
