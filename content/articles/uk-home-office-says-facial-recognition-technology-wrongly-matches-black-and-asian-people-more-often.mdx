---
title: "UK Home Office Says Facial Recognition Technology Wrongly Matches Black and Asian People More Often"
summary: "The UK Home Office says its facial‑recognition system is more likely to wrongly match black and Asian people than white people. Tests by the National Physical Laboratory found false matches of about 5‑6% for black people and 4% for Asians, compared to 0.04% for white people. Police leaders and crime‑commissioners say the bias means the system must be protected or paused. The government will run a 10‑week public consultation before wider use, while a new, less‑biased algorithm is being tested."
category: "Politics"
section: "Technology"
imageUrl: "https://media.guim.co.uk/e24a5e342105469017ec2a3b8a6c952a32266224/551_0_5000_4000/500.jpg"
publishedAt: "2025-12-05T11:11:18Z"
guardianId: "technology/2025/dec/05/home-office-facial-recognition-tech-issue-black-asian-subjects"
---

The UK Home Office says its facial‑recognition system is more likely to wrongly match black and Asian people than white people. Tests by the National Physical Laboratory found false matches of about 5‑6% for black people and 4% for Asians, compared to 0.04% for white people. Police leaders and crime‑commissioners say the bias means the system must be protected or paused. The government will run a 10‑week public consultation before wider use, while a new, less‑biased algorithm is being tested.
