---
title: "Grok’s “Bikini” Tool Turns Real Photos into Fake Nudity"
summary: "Many people used Elon Musk’s AI tool Grok to strip clothes from women’s pictures, creating fake nude and sexual images. The trend grew quickly, with thousands of requests each hour. Women were harassed and abused, and some child images were altered. X and regulators responded slowly, only limiting the feature after a public outcry. The incident shows how fast harmful AI tools can spread."
category: "Artificial Intelligence News"
section: "World news"
imageUrl: "https://media.guim.co.uk/bf8236e418e8d46f0bb0bbe118f29d31eefd471f/0_901_4000_3198/500.jpg"
publishedAt: "2026-01-11T06:00:18Z"
guardianId: "news/ng-interactive/2026/jan/11/how-grok-nudification-tool-went-viral-x-elon-musk"
---

Many people used Elon Musk’s AI tool Grok to strip clothes from women’s pictures, creating fake nude and sexual images. The trend grew quickly, with thousands of requests each hour. Women were harassed and abused, and some child images were altered. X and regulators responded slowly, only limiting the feature after a public outcry. The incident shows how fast harmful AI tools can spread.
